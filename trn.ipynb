{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7153a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************\n",
      "testing model saved:savedModels/20Sep_0911PM_5L_1K_50E_AG/modelTst\n",
      "Reading the data. Please wait...\n",
      "Elapsed time: 2.479124 seconds.\n",
      "\n",
      "Successfully read the data from file!\n",
      "Now doing undersampling....\n",
      "Elapsed time: 15.141680 seconds.\n",
      "\n",
      "Successfully undersampled data!\n",
      "WARNING:tensorflow:From C:\\Users\\Kevin\\AppData\\Local\\Temp/ipykernel_5880/3447821144.py:142: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_initializable_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "WARNING:tensorflow:From C:\\Users\\Kevin\\anaconda3\\envs\\tf 25\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "training started at 20-Sep-2021 09:12 PM\n",
      "parameters are: Epochs: 50  BS: 1 nSteps: 18400 nSamples: 368\n",
      "Model meta graph saved in::savedModels/20Sep_0911PM_5L_1K_50E_AG/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18400/18400 [57:54<00:00,  5.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trianing completed in minutes  58.38426152070363\n",
      "training completed at 20-Sep-2021 10:10 PM\n",
      "*************************************************\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This is the training code to train the model as described in the following article:\n",
    "\n",
    "MoDL: Model-Based Deep Learning Architecture for Inverse Problems\n",
    "by H.K. Aggarwal, M.P. Mani, M. Jacob from University of Iowa.\n",
    "\n",
    "Paper dwonload  Link:     https://arxiv.org/abs/1712.02862\n",
    "\n",
    "This code solves the following optimization problem:\n",
    "\n",
    "    argmin_x ||Ax-b||_2^2 + ||x-Dw(x)||^2_2\n",
    "\n",
    " 'A' can be any measurement operator. Here we consider parallel imaging problem in MRI where\n",
    " the A operator consists of undersampling mask, FFT, and coil sensitivity maps.\n",
    "\n",
    "Dw(x): it represents the residual learning CNN.\n",
    "\n",
    "Here is the description of the parameters that you can modify below.\n",
    "\n",
    "epochs: how many times to pass through the entire dataset\n",
    "\n",
    "nLayer: number of layers of the convolutional neural network.\n",
    "        Each layer will have filters of size 3x3. There will be 64 such filters\n",
    "        Except at the first and the last layer.\n",
    "\n",
    "gradientMethod: MG or AG. set MG for 'manual gradient' of conjuagate gradient (CG) block\n",
    "                as discussed in section 3 of the above paper. Set it to AG if\n",
    "                you want to rely on the tensorflow to calculate gradient of CG.\n",
    "\n",
    "K: it represents the number of iterations of the alternating strategy as\n",
    "    described in Eq. 10 in the paper.  Also please see Fig. 1 in the above paper.\n",
    "    Higher value will require a lot of GPU memory. Set the maximum value to 20\n",
    "    for a GPU with 16 GB memory. Higher the value more is the time required in training.\n",
    "\n",
    "sigma: the standard deviation of Gaussian noise to be added in the k-space\n",
    "\n",
    "batchSize: You can reduce the batch size to 1 if the model does not fit on GPU.\n",
    "\n",
    "Output:\n",
    "\n",
    "After running the code the output model will be saved in the subdirectory 'savedModels'.\n",
    "You can give the name of the generated ouput directory in the tstDemo.py to\n",
    "run the newly trained model on the test data.\n",
    "\n",
    "\n",
    "@author: Hemant Kumar Aggarwal\n",
    "\"\"\"\n",
    "\n",
    "# import some librariesw\n",
    "import os,time\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import supportingFunctions as sf\n",
    "import model as mm\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "#% SET THESE PARAMETERS CAREFULLY\n",
    "nLayers=5\n",
    "epochs=50\n",
    "batchSize=1\n",
    "gradientMethod='AG'\n",
    "K=1\n",
    "sigma=0.02\n",
    "restoreWeights=False\n",
    "#%% to train the model with higher K values  (K>1) such as K=5 or 10,\n",
    "# it is better to initialize with a pre-trained model with K=1.\n",
    "if K>1:\n",
    "    restoreWeights=True\n",
    "    restoreFromModel='04Jun_0243pm_5L_1K_100E_AG'\n",
    "\n",
    "if restoreWeights:\n",
    "    wts=sf.getWeights('savedModels/'+restoreFromModel)\n",
    "#--------------------------------------------------------------------------\n",
    "#%%Generate a meaningful filename to save the trainined models for testing\n",
    "print ('*************************************************')\n",
    "start_time=time.time()\n",
    "saveDir='savedModels/'\n",
    "cwd=os.getcwd()\n",
    "directory=saveDir+datetime.now().strftime(\"%d%b_%I%M%p_\")+ \\\n",
    " str(nLayers)+'L_'+str(K)+'K_'+str(epochs)+'E_'+gradientMethod\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "sessFileName= directory+'/model'\n",
    "\n",
    "\n",
    "#%% save test model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# csmT = tf.placeholder(tf.complex64,shape=(None,12,256,232),name='csm')\n",
    "# maskT= tf.placeholder(tf.complex64,shape=(None,256,232),name='mask')\n",
    "# atbT = tf.placeholder(tf.float32,shape=(None,256,232,2),name='atb')\n",
    "\n",
    "csmT = tf.placeholder(tf.complex64,shape=(None,1,512,512),name='csm')\n",
    "maskT= tf.placeholder(tf.complex64,shape=(None,512,512),name='mask')\n",
    "atbT = tf.placeholder(tf.float32,shape=(None,512,512,2),name='atb')\n",
    "\n",
    "out=mm.makeModel(atbT,csmT,maskT,False,nLayers,K,gradientMethod)\n",
    "predTst=out['dc'+str(K)]\n",
    "predTst=tf.identity(predTst,name='predTst')\n",
    "sessFileNameTst=directory+'/modelTst'\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    savedFile=saver.save(sess, sessFileNameTst,latest_filename='checkpointTst')\n",
    "print ('testing model saved:' +savedFile)\n",
    "#%% read multi-channel dataset\n",
    "trnOrg,trnAtb,trnCsm,trnMask=sf.getData('training')\n",
    "trnOrg,trnAtb=sf.c2r(trnOrg),sf.c2r(trnAtb)\n",
    "\n",
    "#%%\n",
    "tf.reset_default_graph()\n",
    "csmP = tf.placeholder(tf.complex64,shape=(None,None,None,None),name='csm')\n",
    "maskP= tf.placeholder(tf.complex64,shape=(None,None,None),name='mask')\n",
    "atbP = tf.placeholder(tf.float32,shape=(None,None,None,2),name='atb')\n",
    "orgP = tf.placeholder(tf.float32,shape=(None,None,None,2),name='org')\n",
    "\n",
    "\n",
    "#%% creating the dataset\n",
    "nTrn=trnOrg.shape[0]\n",
    "nBatch= int(np.floor(np.float32(nTrn)/batchSize))\n",
    "nSteps= nBatch*epochs\n",
    "\n",
    "trnData = tf.data.Dataset.from_tensor_slices((orgP,atbP,csmP,maskP))\n",
    "trnData = trnData.cache()\n",
    "trnData=trnData.repeat(count=epochs)\n",
    "trnData = trnData.shuffle(buffer_size=trnOrg.shape[0])\n",
    "trnData=trnData.batch(batchSize)\n",
    "trnData=trnData.prefetch(5)\n",
    "iterator=trnData.make_initializable_iterator()\n",
    "orgT,atbT,csmT,maskT = iterator.get_next('getNext')\n",
    "\n",
    "#%% make training model\n",
    "\n",
    "out=mm.makeModel(atbT,csmT,maskT,True,nLayers,K,gradientMethod)\n",
    "predT=out['dc'+str(K)]\n",
    "predT=tf.identity(predT,name='pred')\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.pow(predT-orgT, 2),axis=0))\n",
    "tf.summary.scalar('loss', loss)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gvs = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "    opToRun=optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "\n",
    "#%% training code\n",
    "\n",
    "\n",
    "print ('training started at', datetime.now().strftime(\"%d-%b-%Y %I:%M %p\"))\n",
    "print ('parameters are: Epochs:',epochs,' BS:',batchSize,'nSteps:',nSteps,'nSamples:',nTrn)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "totalLoss,ep=[],0\n",
    "lossT = tf.placeholder(tf.float32)\n",
    "lossSumT = tf.summary.scalar(\"TrnLoss\", lossT)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if restoreWeights:\n",
    "        sess=sf.assignWts(sess,nLayers,wts)\n",
    "\n",
    "    feedDict={orgP:trnOrg,atbP:trnAtb, maskP:trnMask,csmP:trnCsm}\n",
    "    sess.run(iterator.initializer,feed_dict=feedDict)\n",
    "    savedFile=saver.save(sess, sessFileName)\n",
    "    print(\"Model meta graph saved in::%s\" % savedFile)\n",
    "\n",
    "    writer = tf.summary.FileWriter(directory, sess.graph)\n",
    "    for step in tqdm(range(nSteps)):\n",
    "        try:\n",
    "            tmp,_,_=sess.run([loss,update_ops,opToRun])\n",
    "            totalLoss.append(tmp)\n",
    "            if np.remainder(step+1,nBatch)==0:\n",
    "                ep=ep+1\n",
    "                avgTrnLoss=np.mean(totalLoss)\n",
    "                lossSum=sess.run(lossSumT,feed_dict={lossT:avgTrnLoss})\n",
    "                writer.add_summary(lossSum,ep)\n",
    "                totalLoss=[] #after each epoch empty the list of total loos\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    savedfile=saver.save(sess, sessFileName,global_step=ep,write_meta_graph=True)\n",
    "    writer.close()\n",
    "\n",
    "end_time = time.time()\n",
    "print ('Trianing completed in minutes ', ((end_time - start_time) / 60))\n",
    "print ('training completed at', datetime.now().strftime(\"%d-%b-%Y %I:%M %p\"))\n",
    "print ('*************************************************')\n",
    "\n",
    "#%%\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
