{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c67a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code will create the model described in our following paper\n",
    "MoDL: Model-Based Deep Learning Architecture for Inverse Problems\n",
    "by H.K. Aggarwal, M.P. Mani, M. Jacob from University of Iowa.\n",
    "\n",
    "Paper dwonload  Link:     https://arxiv.org/abs/1712.02862\n",
    "\n",
    "@author: haggarwal\n",
    "\"\"\"\n",
    "import tensorflow as tf1\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "epsilon=1e-5\n",
    "TFeps=tf.constant(1e-5,dtype=tf.float32)\n",
    "\n",
    "\n",
    "# function c2r contatenate complex input as new axis two two real inputs\n",
    "c2r=lambda x:tf.stack([tf.real(x),tf.imag(x)],axis=-1)\n",
    "#r2c takes the last dimension of real input and converts to complex\n",
    "r2c=lambda x:tf.complex(x[...,0],x[...,1])\n",
    "\n",
    "def createLayer(x, szW, trainning,lastLayer):\n",
    "    \"\"\"\n",
    "    This function create a layer of CNN consisting of convolution, batch-norm,\n",
    "    and ReLU. Last layer does not have ReLU to avoid truncating the negative\n",
    "    part of the learned noise and alias patterns.\n",
    "    \"\"\"\n",
    "    W=tf.get_variable('W',shape=szW,initializer=tf.contrib.layers.xavier_initializer())\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    xbn=tf.layers.batch_normalization(x,training=trainning,fused=True,name='BN')\n",
    "\n",
    "    if not(lastLayer):\n",
    "        return tf.nn.relu(xbn)\n",
    "    else:\n",
    "        return xbn\n",
    "\n",
    "def dw(inp,trainning,nLay):\n",
    "    \"\"\"\n",
    "    This is the Dw block as defined in the Fig. 1 of the MoDL paper\n",
    "    It creates an n-layer (nLay) residual learning CNN.\n",
    "    Convolution filters are of size 3x3 and 64 such filters are there.\n",
    "    nw: It is the learned noise\n",
    "    dw: it is the output of residual learning after adding the input back.\n",
    "    \"\"\"\n",
    "    lastLayer=False\n",
    "    nw={}\n",
    "    nw['c'+str(0)]=inp\n",
    "    szW={}\n",
    "    szW = {key: (3,3,64,64) for key in range(2,nLay)}\n",
    "    szW[1]=(3,3,2,64)\n",
    "    szW[nLay]=(3,3,64,2)\n",
    "\n",
    "    for i in np.arange(1,nLay+1):\n",
    "        if i==nLay:\n",
    "            lastLayer=True\n",
    "        with tf.variable_scope('Layer'+str(i)):\n",
    "            nw['c'+str(i)]=createLayer(nw['c'+str(i-1)],szW[i],trainning,lastLayer)\n",
    "\n",
    "    with tf.name_scope('Residual'):\n",
    "        shortcut=tf.identity(inp)\n",
    "        dw=shortcut+nw['c'+str(nLay)]\n",
    "    return dw\n",
    "\n",
    "\n",
    "class Aclass:\n",
    "    \"\"\"\n",
    "    This class is created to do the data-consistency (DC) step as described in paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, csm,mask,lam):\n",
    "        with tf.name_scope('Ainit'):\n",
    "            s=tf.shape(mask)\n",
    "            self.nrow,self.ncol=s[0],s[1]\n",
    "            self.pixels=self.nrow*self.ncol\n",
    "            self.mask=mask\n",
    "            self.csm=csm\n",
    "            self.SF=tf.complex(tf.sqrt(tf.to_float(self.pixels) ),0.)\n",
    "            self.lam=lam\n",
    "            #self.cgIter=cgIter\n",
    "            #self.tol=tol\n",
    "    def myAtA(self,img):\n",
    "        with tf.name_scope('AtA'):\n",
    "            coilImages=self.csm*img\n",
    "            kspace=  tf.fft2d(coilImages)/self.SF\n",
    "            temp=kspace*self.mask\n",
    "            coilImgs =tf.ifft2d(temp)*self.SF\n",
    "            coilComb= tf.reduce_sum(coilImgs*tf.conj(self.csm),axis=0)\n",
    "            coilComb=coilComb+self.lam*img\n",
    "        return coilComb\n",
    "\n",
    "def myCG(A,rhs):\n",
    "    \"\"\"\n",
    "    This is my implementation of CG algorithm in tensorflow that works on\n",
    "    complex data and runs on GPU. It takes the class object as input.\n",
    "    \"\"\"\n",
    "    rhs=r2c(rhs)\n",
    "    cond=lambda i,rTr,*_: tf.logical_and( tf.less(i,10), rTr>1e-10)\n",
    "    def body(i,rTr,x,r,p):\n",
    "        with tf.name_scope('cgBody'):\n",
    "            Ap=A.myAtA(p)\n",
    "            alpha = rTr / tf.to_float(tf.reduce_sum(tf.conj(p)*Ap))\n",
    "            alpha=tf.complex(alpha,0.)\n",
    "            x = x + alpha * p\n",
    "            r = r - alpha * Ap\n",
    "            rTrNew = tf.to_float( tf.reduce_sum(tf.conj(r)*r))\n",
    "            beta = rTrNew / rTr\n",
    "            beta=tf.complex(beta,0.)\n",
    "            p = r + beta * p\n",
    "        return i+1,rTrNew,x,r,p\n",
    "\n",
    "    x=tf.zeros_like(rhs)\n",
    "    i,r,p=0,rhs,rhs\n",
    "    rTr = tf.to_float( tf.reduce_sum(tf.conj(r)*r),)\n",
    "    loopVar=i,rTr,x,r,p\n",
    "    out=tf.while_loop(cond,body,loopVar,name='CGwhile',parallel_iterations=1)[2]\n",
    "    return c2r(out)\n",
    "\n",
    "def getLambda():\n",
    "    \"\"\"\n",
    "    create a shared variable called lambda.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "        lam = tf.get_variable(name='lam1', dtype=tf.float32, initializer=.05)\n",
    "    return lam\n",
    "\n",
    "def callCG(rhs):\n",
    "    \"\"\"\n",
    "    this function will call the function myCG on each image in a batch\n",
    "    \"\"\"\n",
    "    G=tf.get_default_graph()\n",
    "    getnext=G.get_operation_by_name('getNext')\n",
    "    _,_,csm,mask=getnext.outputs\n",
    "    l=getLambda()\n",
    "    l2=tf.complex(l,0.)\n",
    "    def fn(tmp):\n",
    "        c,m,r=tmp\n",
    "        Aobj=Aclass(c,m,l2)\n",
    "        y=myCG(Aobj,r)\n",
    "        return y\n",
    "    inp=(csm,mask,rhs)\n",
    "    rec=tf.map_fn(fn,inp,dtype=tf.float32,name='mapFn2' )\n",
    "    return rec\n",
    "\n",
    "@tf.custom_gradient\n",
    "def dcManualGradient(x):\n",
    "    \"\"\"\n",
    "    This function impose data consistency constraint. Rather than relying on\n",
    "    TensorFlow to calculate the gradient for the conjuagte gradient part.\n",
    "    We can calculate the gradient manually as well by using this function.\n",
    "    Please see section III (c) in the paper.\n",
    "    \"\"\"\n",
    "    y=callCG(x)\n",
    "    def grad(inp):\n",
    "        out=callCG(inp)\n",
    "        return out\n",
    "    return y,grad\n",
    "\n",
    "\n",
    "def dc(rhs,csm,mask,lam1):\n",
    "    \"\"\"\n",
    "    This function is called to create testing model. It apply CG on each image\n",
    "    in the batch.\n",
    "    \"\"\"\n",
    "    lam2=tf.complex(lam1,0.)\n",
    "    def fn( tmp ):\n",
    "        c,m,r=tmp\n",
    "        Aobj=Aclass( c,m,lam2 )\n",
    "        y=myCG(Aobj,r)\n",
    "        return y\n",
    "    inp=(csm,mask,rhs)\n",
    "    rec=tf.map_fn(fn,inp,dtype=tf.float32,name='mapFn' )\n",
    "    return rec\n",
    "\n",
    "def makeModel(atb,csm,mask,training,nLayers,K,gradientMethod):\n",
    "    \"\"\"\n",
    "    This is the main function that creates the model.\n",
    "\n",
    "    \"\"\"\n",
    "    out={}\n",
    "    out['dc0']=atb\n",
    "    with tf1.compat.v1.name_scope('myModel'):\n",
    "        with tf1.compat.v1.variable_scope('Wts',reuse=tf.AUTO_REUSE):\n",
    "            for i in range(1,K+1):\n",
    "                j=str(i)\n",
    "                out['dw'+j]=dw(out['dc'+str(i-1)],training,nLayers)\n",
    "                lam1=getLambda()\n",
    "                rhs=atb + lam1*out['dw'+j]\n",
    "                if gradientMethod=='AG':\n",
    "                    out['dc'+j]=dc(rhs,csm,mask,lam1)\n",
    "                elif gradientMethod=='MG':\n",
    "                    if training:\n",
    "                        out['dc'+j]=dcManualGradient(rhs)\n",
    "                    else:\n",
    "                        out['dc'+j]=dc(rhs,csm,mask,lam1)\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
